# ERA1_S6_Assignment
### Part 1: Contains Backpropogation working sheet for part 1 of the assignment
- Part 1 contains, screenshot of the working sheet
-- Backpropogation working sheet screenshot
[!image](https://github.com/Thara1501/ERA1_S6_Assignment/blob/main/Backpropogation.png?raw=true)(https://github.com/Thara1501/ERA1_S6_Assignment/blob/main/Backpropogation.png)
- Excel Working Sheet: [!Backpropogation](https://github.com/Thara1501/ERA1_S6_Assignment/blob/main/BackPropagation_Error%20Plot.xlsx?raw=true)(https://github.com/Thara1501/ERA1_S6_Assignment/blob/main/BackPropagation_Error%20Plot.xlsx)
- Description of each major step:[!nn steps](https://github.com/Thara1501/ERA1_S6_Assignment/blob/main/Neural%20Network%20steps%20until%20backpropogation.txt?raw=true)(https://github.com/Thara1501/ERA1_S6_Assignment/blob/main/Neural%20Network%20steps%20until%20backpropogation.txt)
- Comparison of Error chart for different values of learning rate (0.1,0.2,0.5,0.8,1.0,2,0): Its there in the excel file as a sheet

### Part 2: Ipynb notebook contains NN model for classify MNIST data digits
- Loading modules/libraries
- Define CNN Architecture:
  - Used 2 conv layers plus with relu followed by max polling after each convolutional/relu layer
  - 2 fully connected layers with ReLU activation for the first fully connected later. 
  - Followed by softmax
- Compute the total parameters used
- Define batch size and download the traina nd test set
- Define model training , computation of accuracy and loss for the test set
- Model training and validation
